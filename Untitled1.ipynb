{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c4d08-f64b-4aa3-a5b3-00fd27ae7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense,Dropout, Conv2D, Flatten, MaxPool2D\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2c45a-28e0-4def-a48d-e864778782a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fetch_traffic_images(traffic_path):\n",
    "    num_classes =  43\n",
    "    traffic_images=[]\n",
    "    labels = []\n",
    "    \n",
    "    for label in range(num_classes):\n",
    "        train_folders = os.path.join(traffic_path,'Train',str(label))\n",
    "        imgs = os.listdir(train_folders)\n",
    "        for img in imgs:\n",
    "            image = Image.open(train_folders+'\\\\'+img)\n",
    "            image = image.resize((30,30))\n",
    "            image = np.array(image)\n",
    "            traffic_images.append(image)\n",
    "            labels.append(label)\n",
    "    return traffic_images,labels\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d2b18-ac1d-4626-a1cc-f4a9e6695c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_path = 'C:\\\\Users\\\\Abir\\\\Documents\\\\Image processing\\\\Traffic Signs V1\\\\data'\n",
    "X, Y = fetch_traffic_images(general_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04e0ac-b64a-4f54-9ff5-9bba37024895",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[80]);\n",
    "plt.axis('off')\n",
    "Y = np.array(Y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e72d76-1fe9-45de-98b9-b91848bb7da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(general_path,'Classes.txt')\n",
    "file = open(path)\n",
    "classes = {}\n",
    "for line in file:\n",
    "    data = line.split('-')\n",
    "    classes.update({data[0]:data[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0086a6b0-5862-49df-8b0d-b117701c323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX, trainY,testY = train_test_split(X,Y, test_size=0.2,random_state=42,shuffle = True)\n",
    "print(trainX.shape, testX.shape, trainY.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa0126-06b1-4860-aa93-8304c834fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes[str(Y[1]+1)]\n",
    "length = len(classes)\n",
    "train_y = to_categorical(trainY,length)\n",
    "test_y = to_categorical(testY,length)\n",
    "\n",
    "train_y.shape\n",
    "test_y[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046f9c3-6133-4de2-9926-ffab5362ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(trainX,open(general_path+'\\\\'+'trainX.h5','wb'))\n",
    "pickle.dump(testX,open(general_path+'\\\\'+'testX.h5','wb'))\n",
    "pickle.dump(train_y,open(general_path+'\\\\'+'trainY.h5','wb'))\n",
    "pickle.dump(test_y,open(general_path+'\\\\'+'testY.h5','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231be11-e8dc-4c1b-811d-89152a90120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(inputShape,outputSize):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(5,5), activation='relu', input_shape=inputShape))\n",
    "    model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(5,5), activation='relu'))\n",
    "    model.add(Conv2D(32, kernel_size=(5,5), activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(outputSize,activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "              \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c910c5-22ce-418e-8967-39f6974deb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel(trainX.shape[1:],length)\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d588803-e5ea-4f20-9e7c-fcecd57786de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hist =  model.fit(trainX,train_y, epochs = 16, validation_data=(testX,test_y),batch_size=32)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6967b67-43d2-44c8-8823-031a95074f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(general_path+'\\\\model_traffic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab115be0-e7ba-406e-a57d-88dc14735e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(hist.history,open(general_path+'\\\\hist.h5','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58446f8b-4c15-46c1-a808-c76097cf6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pickle.load(open(general_path+'\\\\trainX.h5','rb'))\n",
    "testX = pickle.load(open(general_path+'\\\\testX.h5','rb'))\n",
    "trainY = pickle.load(open(general_path+'\\\\trainY.h5','rb'))\n",
    "testY = pickle.load(open(general_path+'\\\\testY.h5','rb'))\n",
    "history = pickle.load(open(general_path+'\\\\hist.h5','rb'))\n",
    "\n",
    "model = load_model(general_path+'\\\\model_traffic.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0068858-9d5a-4554-a174-1a834166da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3008731-75cf-4f3e-9c72-6c9b590d124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = model.evaluate(testX,testY)\n",
    "print('Accuracy:',round(value[1]*100,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa3045-f80a-485a-bc2c-fd3380b78217",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(history['accuracy'],label = 'Training Accuracy');\n",
    "plt.plot(history['val_accuracy'],label = 'Val Accuracy');\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy');\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(history['loss'],label = 'Training Loses');\n",
    "plt.plot(history['val_loss'],label = 'Val Loses');\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss');\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8733ad-3d1b-4923-a5a6-693a0785eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv(general_path + '\\\\Test.csv')\n",
    "imgs_path = samples['Path'].values\n",
    "labels = samples['ClassId'].values\n",
    "\n",
    "photos = []\n",
    "\n",
    "for img in imgs_path:\n",
    "    image = Image.open(general_path+'\\\\'+img)\n",
    "    image = image.resize((30,30))\n",
    "    image = np.array(image)\n",
    "    photos.append(image)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e147fe-8b97-4f4a-a6c5-da8cecdf4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(photos)\n",
    "prediction = model.predict(data)\n",
    "pred_index = np.argmax(prediction,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065a7e2-f0df-48a2-b233-56e451bd6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = accuracy_score(labels,pred_index)*100\n",
    "print('Accuracy:',round(value,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40641584-df44-407e-a613-b55f8893491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image2(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((30,30))\n",
    "    image = np.array(image)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    img = np.expand_dims(image, axis=0)\n",
    "    vect_output = model.predict(img)\n",
    "    index = np.argmax(vect_output)\n",
    "    classes[str(index+1)]\n",
    "    definition = classes[str(index+1)]\n",
    "    \n",
    "    print(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bca1d9-7181-401b-b56b-255d7ad14f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Abir\\\\Documents\\\\Image processing\\\\Traffic Signs V1\\\\data\\\\Test\\\\00210.png'\n",
    "classify_image(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af8bc5-408d-478c-8232-5f7c9eda7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gui = tk.Tk()\n",
    "gui.geometry('800x600')\n",
    "gui.title('Traffic-sign recognition')\n",
    "gui.configure(background=\"#ABCDEF\")\n",
    "\n",
    "def classify_image(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((30,30))\n",
    "    image = np.array(image)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    img = np.expand_dims(image, axis=0)\n",
    "    vect_output = model.predict(img)\n",
    "    index = np.argmax(vect_output)\n",
    "    classes[str(index+1)]\n",
    "    definition = classes[str(index+1)]\n",
    "    \n",
    "    traffic_class.configure(text = definition)\n",
    "    \n",
    "def addBtnClassify(path):\n",
    "    btnClassify = tk.Button(gui, text='Classify the image',command = lambda : classify_image(path))\n",
    "    btnClassify.configure(background='black', foreground='white',font=('arial',12,'bold'))\n",
    "    btnClassify.place(relx=0.79,rely=0.49)\n",
    "def uploadImage():\n",
    "    file = filedialog.askopenfilename()\n",
    "    default = Image.open(file)\n",
    "    default = default.resize((150,150))\n",
    "    default = ImageTk.PhotoImage(default)\n",
    "    traffic_image.configure(image=default)\n",
    "    traffic_image.image = default\n",
    "    addBtnClassify(file)\n",
    "\n",
    "btnUpload = tk.Button(gui,command=uploadImage, text='Upload a traffic image')\n",
    "btnUpload.configure(background=\"#AB0000\", foreground='white', font=('arial',12,'bold'))\n",
    "\n",
    "traffic_image = tk.Label(gui)\n",
    "traffic_class = tk.Label(gui,background=\"#ABCDEF\",foreground='#011110',font=('arial',12,'bold'))\n",
    "\n",
    "default = Image.open('C:\\\\Users\\\\Abir\\\\Documents\\\\Image processing\\\\Traffic Signs V1\\\\data\\\\no_image.jpg')\n",
    "default = default.resize((150,150))\n",
    "\n",
    "default = ImageTk.PhotoImage(default)\n",
    "traffic_image.configure(image=default)\n",
    "traffic_image.image = default\n",
    "btnUpload.pack(pady=50)\n",
    "traffic_image.pack()\n",
    "traffic_class.pack()\n",
    "gui.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e021607-7b55-4044-b903-5f3579a9f419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"
     ]
    }
   ],
   "source": [
    "# complete project\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import ImageTk\n",
    "\n",
    "import random\n",
    "\n",
    "general_path = 'C:\\\\Users\\\\Abir\\\\Documents\\\\Image processing\\\\Traffic Signs V1\\\\data'\n",
    "\n",
    "trainX = pickle.load(open(general_path+'\\\\trainX.h5','rb'))\n",
    "testX = pickle.load(open(general_path+'\\\\testX.h5','rb'))\n",
    "trainY = pickle.load(open(general_path+'\\\\trainY.h5','rb'))\n",
    "testY = pickle.load(open(general_path+'\\\\testY.h5','rb'))\n",
    "history = pickle.load(open(general_path+'\\\\hist.h5','rb'))\n",
    "\n",
    "model = load_model(general_path+'\\\\model_traffic.h5')\n",
    "\n",
    "path = os.path.join(general_path,'Classes.txt')\n",
    "file = open(path)\n",
    "classes = {}\n",
    "for line in file:\n",
    "    data = line.split('-')\n",
    "    classes.update({data[0]:data[1]})\n",
    "\n",
    "\n",
    "\n",
    "gui = tk.Tk()\n",
    "gui.geometry('800x600')\n",
    "gui.title('Traffic-sign recognition')\n",
    "gui.configure(background=\"#ABCDEF\")\n",
    "\n",
    "def classify_image(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((30,30))\n",
    "    image = np.array(image)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    img = np.expand_dims(image, axis=0)\n",
    "    vect_output = model.predict(img)\n",
    "    index = np.argmax(vect_output)\n",
    "    classes[str(index+1)]\n",
    "    definition = classes[str(index+1)]\n",
    "    \n",
    "    traffic_class.configure(text = definition)\n",
    "    \n",
    "def addBtnClassify(path):\n",
    "    btnClassify = tk.Button(gui, text='Classify the image',command = lambda : classify_image(path))\n",
    "    btnClassify.configure(background='black', foreground='white',font=('arial',12,'bold'))\n",
    "    btnClassify.place(relx=0.79,rely=0.49)\n",
    "def uploadImage():\n",
    "    file = filedialog.askopenfilename()\n",
    "    default = Image.open(file)\n",
    "    default = default.resize((150,150))\n",
    "    default = ImageTk.PhotoImage(default)\n",
    "    traffic_image.configure(image=default)\n",
    "    traffic_image.image = default\n",
    "    addBtnClassify(file)\n",
    "\n",
    "def chooseImage():\n",
    "    file = random.choice(imgs_path)\n",
    "    default = Image.open(general_path + '\\\\' + file)\n",
    "    default = default.resize((150,150))\n",
    "    default = ImageTk.PhotoImage(default)\n",
    "    traffic_image.configure(image=default)\n",
    "    traffic_image.image = default\n",
    "    addBtnClassify(general_path + '\\\\' + file)\n",
    "\n",
    "btnUpload = tk.Button(gui,command=uploadImage, text='Upload a traffic image')\n",
    "btnUpload.configure(background=\"#AB0000\", foreground='white', font=('arial',12,'bold'))\n",
    "\n",
    "btnRandom = tk.Button(gui,command=chooseImage, text='Random Choice')\n",
    "btnRandom.configure(background=\"black\", foreground='white', font=('arial',12,'bold'))\n",
    "\n",
    "traffic_image = tk.Label(gui)\n",
    "traffic_class = tk.Label(gui,background=\"#ABCDEF\",foreground='#011110',font=('arial',12,'bold'))\n",
    "\n",
    "default = Image.open('C:\\\\Users\\\\Abir\\\\Documents\\\\Image processing\\\\Traffic Signs V1\\\\data\\\\no_image.jpg')\n",
    "default = default.resize((150,150))\n",
    "\n",
    "default = ImageTk.PhotoImage(default)\n",
    "traffic_image.configure(image=default)\n",
    "traffic_image.image = default\n",
    "btnUpload.pack(pady=50)\n",
    "traffic_image.pack()\n",
    "btnRandom.pack(pady=50)\n",
    "traffic_class.pack()\n",
    "gui.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bc9f7-c62f-4b93-95db-43daacadb892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.choice(imgs_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
